import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

class KeywordExtractor:
    def __init__(self):
        # Turkish stopwords
        self.stopwords = set([
            've', 'bir', 'bu', 'da', 'de', 'iÃ§in', 'ile', 'olan', 'olarak', 
            'sonra', 'sonrasÄ±', 'mezuniyet', 'program', 'programÄ±', 'bÃ¶lÃ¼m',
            'alan', 'alanÄ±nda', 'konusunda', 'hakkÄ±nda', 'gibi', 'kadar',
            'Ã§alÄ±ÅŸma', 'Ã§alÄ±ÅŸÄ±r', 'Ã¶ÄŸrenci', 'Ã¶ÄŸrenciler', 'ders', 'dersler'
        ])
        
        # Interest area mappings
        self.interest_keywords = {
            'teknoloji': ['teknoloji', 'bilgisayar', 'yazÄ±lÄ±m', 'dijital', 'elektronik', 'otomasyon', 'robot'],
            'sanat': ['sanat', 'tasarÄ±m', 'yaratÄ±cÄ±', 'estetik', 'gÃ¶rsel', 'grafik', 'mÃ¼zik'],
            'saÄŸlÄ±k': ['saÄŸlÄ±k', 'hasta', 'tedavi', 'tÄ±p', 'ameliyat', 'bakÄ±m', 'hemÅŸire', 'doktor'],
            'matematik': ['matematik', 'hesap', 'analiz', 'istatistik', 'sayÄ±sal', 'formÃ¼l'],
            'iletiÅŸim': ['iletiÅŸim', 'sosyal', 'insan', 'toplum', 'dil', 'konuÅŸma', 'medya'],
            'spor': ['spor', 'fitness', 'antrenÃ¶r', 'egzersiz', 'hareket', 'atletik'],
            'doÄŸa': ['tarÄ±m', 'Ã§evre', 'doÄŸa', 'bitki', 'hayvan', 'ekoloji', 'orman'],
            'gÃ¼venlik': ['gÃ¼venlik', 'polis', 'askeriye', 'koruma', 'emniyet', 'kurtarma'],
            'iÅŸletme': ['iÅŸletme', 'yÃ¶netim', 'ekonomi', 'ticaret', 'pazarlama', 'satÄ±ÅŸ'],
            'mÃ¼hendislik': ['mÃ¼hendislik', 'teknik', 'proje', 'inÅŸaat', 'yapÄ±', 'sistem']
        }
    
    def clean_text(self, text):
        """Metni temizle ve normalize et"""
        if not text or text == '-':
            return ""
        
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        text = text.lower()
        
        # Ã–zel karakterleri temizle
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def extract_keywords_from_description(self, description):
        """AÃ§Ä±klamadan keywords Ã§Ä±kar"""
        cleaned = self.clean_text(description)
        if not cleaned:
            return []
        

        words = cleaned.split()
        
        # Stopwords'leri filtrele ve 2+ karakter olanlarÄ± al
        keywords = [word for word in words 
                   if word not in self.stopwords and len(word) > 2]
        
        return keywords
    
    def map_to_interests(self, keywords):
        """Keywords'leri ilgi alanlarÄ±na map et"""
        interests = []
        keyword_str = ' '.join(keywords)
        
        for interest, interest_keywords in self.interest_keywords.items():
            for keyword in interest_keywords:
                if keyword in keyword_str:
                    interests.append(interest)
                    break
        
        return list(set(interests))  # Unique interests
    
    def categorize_by_ranking(self, ranking):
        """SÄ±ralamaya gÃ¶re zorluk kategorisi"""
        if not ranking or ranking == '-':
            return 'unknown'
        
        try:
            rank = float(str(ranking).replace(',', '.'))
            if rank < 100000:
                return 'zor'
            elif rank < 400000:
                return 'orta'
            else:
                return 'kolay'
        except:
            return 'unknown'
    
    def process_dataset(self, csv_path):
        """Dataset'i iÅŸle ve training data hazÄ±rla"""
        df = pd.read_csv(csv_path)
        
        training_data = []
        
        # Unique bÃ¶lÃ¼mleri al
        unique_departments = df.drop_duplicates(subset=['bolum_adi'])
        
        for _, row in unique_departments.iterrows():
            # Keywords Ã§Ä±kar
            keywords = self.extract_keywords_from_description(row['Aciklama'])
            interests = self.map_to_interests(keywords)
            
            if not interests:  # Ä°lgi alanÄ± bulunamazsa skip
                continue
            
           # YENÄ° KOD:
            ranking_2025 = row.get('2025_Taban_SÄ±ralama', 0)

# Training data oluÅŸtur
            training_sample = {
                'bolum_adi': row['bolum_adi'],
                'interests': interests,
                'keywords': keywords[:10],
                'ranking_2025': ranking_2025,
                'universite': row.get('Universite', ''),
                'sehir': row.get('Sehir', ''),
                'description': row['Aciklama'][:200] + '...' if len(str(row['Aciklama'])) > 200 else row['Aciklama']
            }
            
            training_data.append(training_sample)
        
        return training_data
    
    def save_training_data(self, training_data, output_path):
        """Training data'yÄ± JSON olarak kaydet"""
        import json
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Training data saved: {output_path}")
        print(f"ğŸ“Š Total samples: {len(training_data)}")

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    extractor = KeywordExtractor()
    
    # Dataset'i iÅŸle
    training_data = extractor.process_dataset('/Users/ardaerdegirmenci/Desktop/Pupilica/kuzular/Dataset_creation/Datasets/2yillik_Bolumler_aciklamali_yeni.csv')
    
    # SonuÃ§larÄ± kaydet
    extractor.save_training_data(training_data, '/Users/ardaerdegirmenci/Desktop/Pupilica/kuzular/Dataset_creation/Datasets/extracted_keywords.json')
    
    # Ã–rnek sonuÃ§larÄ± gÃ¶ster
    print("\nğŸ“‹ Sample Results:")
    for i, sample in enumerate(training_data[:5]):
        print(f"\n{i+1}. {sample['bolum_adi']}")
        print(f"   Ä°lgi AlanlarÄ±: {sample['interests']}")
        print(f"   Keywords: {sample['keywords'][:5]}")
